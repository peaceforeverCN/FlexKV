#!/usr/bin/env python3
"""
Replay script for FlexKV trace files

This script can read trace files generated by FlexKV tracer and replay the entire
workflow including:
1. Configuration setup (model_config, cache_config, gpu_layout)
2. GPU blocks creation
3. KVManager initialization
4. Request replay (get_async, put_async)
5. Wait operations replay

Usage:
    python replay_from_tracer.py --trace-file ./flexkv_trace.log --verbose
"""

import argparse
import json
import sys
import time
from typing import Dict, List, Optional, Any, Tuple
import torch

from flexkv.common.config import CacheConfig, ModelConfig
from flexkv.common.storage import KVCacheLayout, KVCacheLayoutType
from flexkv.common.memory_handle import TensorSharedHandle
from flexkv.kvtask import KVTaskEngine


class FlexKVReplayEngine:
    """Engine for replaying FlexKV trace files"""

    def __init__(self, trace_file: str, verbose: bool = True, gpu_blocks_num: int = 1000):
        self.trace_file = trace_file
        self.verbose = verbose
        self.events = []
        self.model_config = None
        self.cache_config = None
        self.gpu_layout = None
        self.kvmanager = None
        self.gpu_blocks_num = gpu_blocks_num
        self.gpu_blocks = {}

    def log(self, message: str):
        """Log message if verbose mode is enabled"""
        if self.verbose:
            print(f"[REPLAY] {message}")

    def load_trace_file(self):
        """Load and parse the trace file"""
        self.log(f"Loading trace file: {self.trace_file}")

        try:
            with open(self.trace_file, encoding='utf-8') as f:
                lines = f.readlines()

            self.events = []
            for line_num, line in enumerate(lines, 1):
                try:
                    event = json.loads(line.strip())
                    self.events.append(event)
                except json.JSONDecodeError as e:
                    print(f"Warning: Failed to parse line {line_num}: {e}")
                    continue

            self.log(f"Loaded {len(self.events)} events from trace file")

        except FileNotFoundError:
            print(f"Error: Trace file '{self.trace_file}' not found")
            sys.exit(1)
        except Exception as e:
            print(f"Error loading trace file: {e}")
            sys.exit(1)

    def parse_config_event(self, event: Dict[str, Any]):
        """Parse configuration event and setup configs"""
        self.log("Parsing configuration...")

        data = event['data']
        model_config_data = data['model_config']
        cache_config_data = data['cache_config']
        gpu_layout_data = data.get('gpu_layout')

        # Recreate model_config
        dtype_str = model_config_data['dtype']
        if dtype_str == "torch.float16":
            dtype = torch.float16
        elif dtype_str == "torch.float32":
            dtype = torch.float32
        elif dtype_str == "torch.bfloat16":
            dtype = torch.bfloat16
        else:
            dtype = torch.float16  # default

        self.model_config = ModelConfig(
            num_layers=model_config_data['num_layers'],
            num_kv_heads=model_config_data['num_kv_heads'],
            head_size=8,#model_config_data['head_size'], # for local test
            use_mla=model_config_data['use_mla'],
            dtype=dtype,
            tp_size=1,#model_config_data['tp_size'], # for local test
            dp_size=1,#model_config_data['dp_size'], # for local test
        )

        # Recreate cache_config (with trace disabled for replay)
        self.cache_config = CacheConfig(
            tokens_per_block=cache_config_data['tokens_per_block'],
            enable_cpu=cache_config_data['enable_cpu'],
            enable_ssd=cache_config_data['enable_ssd'],
            enable_remote=cache_config_data['enable_remote'],
            gpu_kv_layout_type=self._parse_layout_type(cache_config_data['gpu_kv_layout_type']),
            cpu_kv_layout_type=self._parse_layout_type(cache_config_data['cpu_kv_layout_type']),
            ssd_kv_layout_type=self._parse_layout_type(cache_config_data['ssd_kv_layout_type']),
            remote_kv_layout_type=self._parse_layout_type(cache_config_data['remote_kv_layout_type']),
            use_gds=cache_config_data['use_gds'],
            remote_cache_size_mode=cache_config_data['remote_cache_size_mode'],
            num_cpu_blocks=cache_config_data['num_cpu_blocks'],
            num_ssd_blocks=cache_config_data['num_ssd_blocks'],
            num_remote_blocks=cache_config_data['num_remote_blocks'],
            remote_file_size=cache_config_data['remote_file_size'],
            remote_file_num=cache_config_data['remote_file_num'],
            remote_file_prefix=cache_config_data['remote_file_prefix'],
            ssd_cache_dir=cache_config_data['ssd_cache_dir'],
            ssd_cache_iouring_entries=cache_config_data['ssd_cache_iouring_entries'],
            ssd_cache_iouring_flags=cache_config_data['ssd_cache_iouring_flags'],
            remote_cache_path=cache_config_data['remote_cache_path'],
            remote_config_custom=cache_config_data['remote_config_custom'],
            enable_trace=False,  # Disable trace for replay
        )

        # Recreate gpu_layout if available
        if gpu_layout_data:
            self.gpu_layout = KVCacheLayout(
                type=self._parse_layout_type(gpu_layout_data['type']),
                num_layer=gpu_layout_data['num_layer'],
                num_block=gpu_layout_data['num_block'],
                tokens_per_block=gpu_layout_data['tokens_per_block'],
                num_head=gpu_layout_data['num_head'],
                head_size=8,#gpu_layout_data['head_size'], #for local test
                is_mla=gpu_layout_data['is_mla'],
            )

        self.gpu_blocks_num = self.gpu_layout.num_block

        self.log(f"Model config: {self.model_config}")
        self.log(f"Cache config loaded {self.cache_config}")
        if self.gpu_layout:
            self.log(f"GPU layout: {self.gpu_layout}")

    def _parse_layout_type(self, layout_type_str: str) -> KVCacheLayoutType:
        """Parse layout type string to enum"""
        if "LAYERWISE" in layout_type_str:
            return KVCacheLayoutType.LAYERWISE
        elif "BLOCKWISE" in layout_type_str:
            return KVCacheLayoutType.BLOCKWISE
        else:
            return KVCacheLayoutType.LAYERWISE  # default

    def create_gpu_blocks(self):
        """Create GPU blocks for testing (similar to test code)"""
        self.log("Creating GPU blocks...")

        total_gpus = self.model_config.tp_size * self.model_config.dp_size
        available_gpus = torch.cuda.device_count()

        if available_gpus < total_gpus:
            self.log(f"Warning: Need {total_gpus} GPUs but only {available_gpus} available. Using GPU 0 for all.")

        self.gpu_blocks = {}
        for gpu_id in range(total_gpus):
            self.gpu_blocks[gpu_id] = []
            # Use available GPU or fallback to GPU 0
            device_name = f"cuda:{gpu_id}" if gpu_id < available_gpus else "cuda:0"

            for layer_id in range(self.model_config.num_layers):
                # Create KV cache tensor: [2, num_blocks, tokens_per_block, num_heads, head_size]
                kv_dim = 2 if not self.model_config.use_mla else 1
                kv_tensor = torch.zeros(  # Use zeros instead of random for reproducibility
                    size=(kv_dim, self.gpu_blocks_num, self.cache_config.tokens_per_block,
                          self.model_config.num_kv_heads // self.model_config.tp_size,
                          self.model_config.head_size),
                    dtype=self.model_config.dtype,
                    device=device_name
                )
                self.gpu_blocks[gpu_id].append(kv_tensor)

        self.log(f"Created GPU blocks for {total_gpus} GPUs with {self.gpu_blocks_num} blocks each")

    def create_kvmanager(self,):
        """Create and initialize KVManager"""
        self.log("Creating KVManager...")

        if not self.gpu_layout:
            # Create default GPU layout if not provided in trace
            self.gpu_layout = KVCacheLayout(
                type=KVCacheLayoutType.LAYERWISE,
                num_layer=self.model_config.num_layers,
                num_block=self.gpu_blocks_num,  # default number of blocks
                tokens_per_block=self.cache_config.tokens_per_block,
                num_head=self.model_config.num_kv_heads // self.model_config.tp_size,
                head_size=self.model_config.head_size,
                is_mla=self.model_config.use_mla
            )

        # Create KVManager
        self.kvmanager = KVTaskEngine(
            model_config=self.model_config,
            cache_config=self.cache_config,
            gpu_layout=self.gpu_layout,
            gpu_blocks=self.gpu_blocks
        )

        # Start KVManager
        if self.kvmanager.is_ready():
            self.kvmanager.start()
            self.log("KVManager started successfully")
        else:
            raise RuntimeError("KVManager is not ready")

    def replay_request_event(self, event: Dict[str, Any]) -> int:
        """Replay a request event (GET or PUT)"""
        data = event['data']
        request_type = data['request_type']

        # Convert lists back to tensors
        token_ids = torch.tensor(data['token_ids'], dtype=torch.long)
        slot_mapping = torch.tensor(data['slot_mapping'], dtype=torch.long)
        token_mask = torch.tensor(data['token_mask'], dtype=torch.bool) if data['token_mask'] else None
        layer_granularity = data.get('layer_granularity', -1)
        dp_id = data.get('dp_id', 0)

        self.log(f"Replaying {request_type} request with {len(token_ids)} tokens")

        if request_type == "GET":
            print(f"üîçüîçüîçGET token_ids: {token_ids[:128]}")
            print(f"request_id: {data['request_id']}, request_type: {request_type}, "
                  f"input length: {len(token_ids)}, true in mask: {token_mask.sum()}")
            task_id = self.kvmanager.get_async(
                token_ids=token_ids,
                slot_mapping=slot_mapping,
                token_mask=token_mask,
                layer_granularity=layer_granularity,
                dp_id=dp_id
            )
        elif request_type == "PUT":
            print(f"‚úÖ‚úÖ‚úÖPUT token_ids: {token_ids[:128]}")
            print(f"request_id: {data['request_id']}, request_type: {request_type}, "
                  f"input length: {len(token_ids)}, true in mask: {token_mask.sum()}")
            task_id = self.kvmanager.put_async(
                token_ids=token_ids,
                slot_mapping=slot_mapping,
                token_mask=token_mask,
                dp_id=dp_id
            )
        else:
            raise ValueError(f"Unknown request type: {request_type}")

        return task_id

    def replay_wait_event(self, event: Dict[str, Any]):
        """Replay a wait event"""
        data = event['data']
        wait_type = data['wait_type']
        task_ids = data['task_ids']
        layer_group_id = data.get('layer_group_id')

        self.log(f"‚è∞‚è∞‚è∞Replaying {wait_type} for task_ids: {task_ids}")

        try:
            if wait_type == "wait":
                result = self.kvmanager.wait(task_ids)
            elif wait_type == "wait_for_graph_finished":
                result = self.kvmanager.wait_for_graph_finished(task_ids)
            elif wait_type == "try_wait":
                result = self.kvmanager.try_wait(task_ids)
            else:
                raise ValueError(f"Unknown wait type: {wait_type}")
            successed_elements = []
            for task_id in task_ids:
                successed_elements.append(result[task_id].sum().item())
            print(f"wait result: task ids: {task_ids}, successed elements num: {successed_elements}")
            self.log(f"Wait completed successfully for {wait_type}")
            return result

        except Exception as e:
            self.log(f"Warning: Wait operation failed: {e}")
            return None

    def replay_all_events(self):
        """Replay all events in chronological order"""
        self.log("Starting event replay...")

        config_events = [e for e in self.events if e['event_type'] == 'config']
        request_events = [e for e in self.events if e['event_type'] == 'request']
        wait_events = [e for e in self.events if e['event_type'] == 'wait']

        self.log(f"Found {len(config_events)} config, {len(request_events)} request, {len(wait_events)} wait events")

        # Parse configuration first
        if config_events:
            self.parse_config_event(config_events[0])
            print("parse_config_event done")
        else:
            raise ValueError("No configuration found in trace file")

        # Create GPU blocks and KVManager
        self.create_gpu_blocks()
        self.create_kvmanager()
        # Replay all non-config events in timestamp order
        other_events = request_events + wait_events
        other_events.sort(key=lambda e: e['timestamp'])

        request_id_mapping = {}  # Map original request_id to replayed task_id
        for replayed_event_num, event in enumerate(other_events):
            event_type = event['event_type']

            if event_type == 'request':
                original_request_id = event['data']['request_id']
                replayed_task_id = self.replay_request_event(event)
                request_id_mapping[original_request_id] = replayed_task_id
                self.log(f"Mapped original request_id {original_request_id} to task_id {replayed_task_id}")

            elif event_type == 'wait':
                # Map original task_ids to replayed task_ids
                original_task_ids = event['data']['task_ids']
                mapped_task_ids = []
                for orig_id in original_task_ids:
                    if orig_id in request_id_mapping:
                        mapped_task_ids.append(request_id_mapping[orig_id])
                    else:
                        self.log(f"Warning: Cannot find mapping for task_id {orig_id}")
                        mapped_task_ids.append(orig_id)  # Use original if not found

                # Update event data with mapped task_ids
                event['data']['task_ids'] = mapped_task_ids
                results = self.replay_wait_event(event)
                print("wait request done")

            # Small delay between events to simulate real timing
            time.sleep(0.001)
            #if replayed_event_num == 200:
            #    break

        self.log("Event replay completed successfully!")

    def cleanup(self):
        """Cleanup resources"""
        if self.kvmanager:
            self.kvmanager.shutdown()
            self.log("KVManager shutdown completed")


def main():
    parser = argparse.ArgumentParser(description="Replay FlexKV trace files")
    parser.add_argument("--trace-file", required=True, help="Path to the trace file")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
    parser.add_argument("--gpu-blocks", type=int, default=1000, help="Number of GPU blocks to create")

    args = parser.parse_args()

    try:
        replay_engine = FlexKVReplayEngine(args.trace_file, args.verbose, args.gpu_blocks)
        replay_engine.load_trace_file()
        replay_engine.replay_all_events()

        print("‚úÖ Replay completed successfully!")

    except KeyboardInterrupt:
        print("\n‚ùå Replay interrupted by user")
    except Exception as e:
        print(f"‚ùå Replay failed: {e}")
        sys.exit(1)
    finally:
        if 'replay_engine' in locals():
            replay_engine.cleanup()


if __name__ == "__main__":
    main()
